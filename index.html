<!DOCTYPE html>
<html>
  <head>
    <title>Yiran Wang's Homepage</title>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- <link rel="shortcut icon" href="./img/favicon.ico"> -->
    <link rel="stylesheet" href="./css/w3.css">
    <link rel="stylesheet" href="./css/fonts.css">
    
    <link rel="stylesheet" href="./css/mycss.css">
    
    <!-- <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css"> -->
    <script src="https://kit.fontawesome.com/abfd71ef95.js" crossorigin="anonymous"></script>
    <style>
    html,body,h1,h2,h3,h4,h5,h6 {font-family: "Roboto", sans-serif}
    </style>
  </head>
<body class="w3-light-grey">

<!-- Page Container -->
<div class="w3-content w3-margin-top" style="max-width:1400px;">

  <!-- The Grid -->
  <div class="w3-row-padding">
  
    <!-- Left Column -->
    <div class="w3-third">
    
      <div class="w3-white w3-text-grey w3-card-4">
        <div class="w3-display-container">
          <img src="./zhuzhu.jpg" style="width:100%" alt="Avatar">
          <div class="w3-display-bottomleft w3-container w3-text-white">
            <h2>Yiran Wang (王一然)</h2>
          </div>
        </div>
        <div class="w3-container">
          <p><i class="fa fa-briefcase fa-fw w3-margin-right w3-large w3-text-blue-grey"></i>PhD student</p>
          <p><i class="fa fa-home fa-fw w3-margin-right w3-large w3-text-blue-grey"></i>Wuhan, China</p>
          <p><i class="fa fa-envelope fa-fw w3-margin-right w3-large w3-text-blue-grey"></i><a href = "mailto:wangyiran@hust.edu.cn">Email</a></p>
          <p><i class="fa fa-github fa-fw w3-margin-right w3-large w3-text-blue-grey"></i><a href="https://github.com/RaymondWang987" target="_blank">Github</a></p>
          <p><i class="fa fa-google fa-fw w3-margin-right w3-large w3-text-blue-grey"></i><a href="https://scholar.google.com/citations?user=p_RnaI8AAAAJ&hl=zh-CN" target="_blank">Google Scholar</a></p>
          <hr>

          <p class="w3-large"><b>Research Interests</b></p>
          <ul class="edu-ul">
            <li>Computer Vision</li>
            <li>Video Temporal Consistency</li>
            <li>Dense Prediction</li>
            <li>Depth Estimation</li>
            <li>Semantic Segmentation</li>
            <li>Autonomous Driving</li>
          </ul>
          <!-- <hr> -->
          <br>

          <!-- <div style="display:none"><script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=a&t=m&d=oclZz2-KQhRpM7TF87e27kJ2BWA6IU8wm8UAW-WAGXY&co=2d78ad&cmo=3acc3a&cmn=ff5353&ct=ffffff'></script></div> -->

          <!-- <div><a href='https://clustrmaps.com/site/1bqzz'  title='Visit tracker'><img src='//clustrmaps.com/map_v2.png?cl=ffffff&w=480&t=m&d=oclZz2-KQhRpM7TF87e27kJ2BWA6IU8wm8UAW-WAGXY' style="width: 100%;";/></a></div> -->
          <!-- <p class="w3-large"><b><i class="fa fa-asterisk fa-fw w3-margin-right w3-text-blue-grey"></i>Skills</b></p>
          <p>Adobe Photoshop</p>
          <div class="w3-light-grey w3-round-xlarge w3-small">
            <div class="w3-container w3-center w3-round-xlarge w3-blue-grey" style="width:90%">90%</div>
          </div>
          <p>Photography</p>
          <div class="w3-light-grey w3-round-xlarge w3-small">
            <div class="w3-container w3-center w3-round-xlarge w3-blue-grey" style="width:80%">
              <div class="w3-center w3-text-white">80%</div>
            </div>
          </div>
          <p>Illustrator</p>
          <div class="w3-light-grey w3-round-xlarge w3-small">
            <div class="w3-container w3-center w3-round-xlarge w3-blue-grey" style="width:75%">75%</div>
          </div>
          <p>Media</p>
          <div class="w3-light-grey w3-round-xlarge w3-small">
            <div class="w3-container w3-center w3-round-xlarge w3-blue-grey" style="width:50%">50%</div>
          </div>
          <br>

          <p class="w3-large w3-text-theme"><b><i class="fa fa-globe fa-fw w3-margin-right w3-text-blue-grey"></i>Languages</b></p>
          <p>English</p>
          <div class="w3-light-grey w3-round-xlarge">
            <div class="w3-round-xlarge w3-blue-grey" style="height:24px;width:100%"></div>
          </div>
          <p>Spanish</p>
          <div class="w3-light-grey w3-round-xlarge">
            <div class="w3-round-xlarge w3-blue-grey" style="height:24px;width:55%"></div>
          </div>
          <p>German</p>
          <div class="w3-light-grey w3-round-xlarge">
            <div class="w3-round-xlarge w3-blue-grey" style="height:24px;width:25%"></div>
          </div>
          <br> -->
        </div>
      </div><br>

    <!-- End Left Column -->
    </div>

    <!-- Right Column -->
    <div class="w3-twothird">


      <div class="w3-container w3-card w3-white w3-margin-bottom">
        <h2 class="w3-text-grey w3-padding-16"><i class="fa-solid fa-heart fa-fw w3-margin-right w3-xxlarge w3-text-blue-grey"></i>Biography</h2>
        <div class="w3-container">
          <ul class="w3-ul">
            I am a final-year Ph.D. student at Huazhong University of Science and Technology (HUST) supervised by Prof. <a href='http://faculty.hust.edu.cn/caozhiguo1/en/index.htm'>Zhiguo Cao</a> (expected to graduate in June 2026). My research interests include 3D computer vision, video temporal consistency, dense prediction, and autonomous driving. 
            From Nov. 2023 to Nov. 2024, I work as a visiting student at Prof. <a href='https://guosheng.github.io/'>Guosheng Lin</a>'s group in <a href='https://www.ntu.edu.sg/s-lab'>S-Lab@NTU</a>, Singapore. From APR. 2022 to Aug. 2024, I cooperate with Dr. <a href='https://jimmie33.github.io/'>Jianming Zhang</a> in Adobe Research. From Feb. 2025 till Aug. 2025, I work with Dr. <a href='https://xiaohangzhan.github.io/'>Xiaohang Zhan</a> and Prof. <a href='https://scholar.google.com.hk/citations?user=tMY31_gAAAAJ&hl=zh-CN&oi=ao'>Dong Yu</a> in Tencent AI Lab as a research intern.
          </ul>
          <ul class="w3-ul">
            <b>I am actively seeking full-time positions starting from 2026</b>.
          </ul>
          <br>
        </div>
      </div>
      
      <div class="w3-container w3-card w3-white w3-margin-bottom">
        <h2 class="w3-text-grey w3-padding-16"><i class="fa fa-fire fa-fw w3-margin-right w3-xxlarge w3-text-blue-grey"></i>News</h2>
        <div class="w3-container">
          <ul class="w3-ul">
            <li>[ 2025-10 ] I am awarded the 2025 National Scholarship for Doctoral Students (two consecutive years). </li>
            <li>[ 2025-05 ] I am selected as CVPR 2025 Outstanding Reviewer (5%).
            <li>[ 2025-04 ] <a href='https://arxiv.org/abs/2504.11773v1'>TacoDepth</a> is selected as CVPR 2025 Best Paper Award Candidate (15/13008, 0.1%).
            <li>[ 2025-04 ] <a href='https://arxiv.org/abs/2504.11773v1'>TacoDepth</a> is selected as CVPR 2025 Oral (96/13008, 0.7%) with the final rating of 5554.
            <li>[ 2025-04 ] CH3Depth is selected as CVPR 2025 Highlight (387/13008, 2.9%).
            <li>[ 2025-02 ] <a href='https://arxiv.org/abs/2504.11773v1'>TacoDepth</a> and CH3Depth are accepted by CVPR 2025. See you in Nashville! </li>
            <li>[ 2024-10 ] I am awarded the 2024 National Scholarship for Doctoral Students. </li>
            <li>[ 2024-10 ] <a href='https://arxiv.org/pdf/2307.08695'>NVDS+</a> is accepted by TPAMI 2024!</li>
            <li>[ 2024-09 ] <a href='https://arxiv.org/abs/2409.17880'>SDDR</a> is accepted by NeurIPS 2024. See you in Vancouver!</li>
            <li>[ 2024-07 ] Winner Award in "ECCV 2024 TRICKY challenge on Monocular Depth of Specular and Transparent Surfaces"</li>
            <li>[ 2024-06 ] Winner Award in "CVPR 2024 NTIRE challenge on HR depth from images of specular and transparent surfaces"</li>
            <li>[ 2023-07 ] <a href='https://arxiv.org/abs/2308.02283'>DADP</a> is accepted by ACM MM 2023.</li> 
            <li>[ 2023-05 ] <a href='https://openaccess.thecvf.com/content/ICCV2023/html/Wang_Neural_Video_Depth_Stabilizer_ICCV_2023_paper.html'>NVDS</a> is accepted by ICCV 2023.</li>
            <li>[ 2022-07 ] <a href='https://arxiv.org/pdf/2208.00380'>FMNet</a> is accepted by ACM MM 2022.</li>
            <li>[ 2022-06 ] One paper is accepted by ACCV 2022 as oral presentation.</li>
            <li>[ 2021-06 ] Runner Up Award in "CVPR 2021 Mobile AI challenge on fast and accurate single-image depth estimation"</li>
            <li>[ 2021-06 ] One paper is accepted by CVPR 2021 Workshops.</li>
          </ul>
          <br>
        </div>
      </div>



      <div class="w3-container w3-card w3-white w3-margin-bottom">
        <h2 class="w3-text-grey w3-padding-16"><i class="fa fa-building-columns fa-fw w3-margin-right w3-xxlarge w3-text-blue-grey"></i>Educations and Experiences</h2>
        <div class="w3-container">
          <div class="pub-figure">
            <img src="./img/hust-logo.png">
          </div>
          <div class="pub-text">
            <div class="info">
              <h5 class="w3-opacity"><b>Ph.D., Huazhong University of Science and Technology</b></h5>
              <h6 class="w3-text-blue-grey"><i class="fa fa-calendar fa-fw w3-margin-right"></i>Sep. 2021 - Jun. 2026</h6>
              <ul class="edu-ul">
                <li><b>Major</b>: Artificial Intelligence, School of Artificial Intelligence and Automation</li>
                <li><b>Supervisor</b>: Prof. <a href="http://english.aia.hust.edu.cn/info/1085/1528.htm" target="_blank">Zhiguo Cao</a></li>
                <li><b>Research Topics</b>: Depth Estimation, Video Temporal Consistency, Dense Prediction, Semantic Segmentation, Light-weight Models, Knowledge Distillation
                <li><b>Honors and Awards</b>: Outstanding Graduate Student, Outstanding Freshman, etc.</li>
              </ul>
            </div>
          </div>
        </div>
        <hr>
        <div class="w3-container">
          <div class="pub-figure">
            <img src="./img/hust-logo.png">
          </div>
          <div class="pub-text">
            <div class="info">
              <h5 class="w3-opacity"><b>B.Eng., Huazhong University of Science and Technology</b></h5>
              <h6 class="w3-text-blue-grey"><i class="fa fa-calendar fa-fw w3-margin-right"></i>Sep. 2017 - Jun. 2021</h6>
              <ul class="edu-ul">
                <li><b>Major</b>: Automation Experimental Class, School of Artificial Intelligence and Automation</li>
                <li><b>Honors and Awards</b>: Outstanding Graduates, Academic Scholarship, etc.</li>
                <!-- Merit Student, Outstanding Graduates -->
              </ul>
            </div>
          </div>
        </div>
        <hr>
        <div class="w3-container">
          <div class="pub-figure">
            <img src="./img/ntu_logo.webp">
          </div>
          <div class="pub-text">
            <div class="info">
              <h5 class="w3-opacity"><b>Visiting Student, NTU and Sensetime Research (S-Lab)</b></h5>
              <h6 class="w3-text-blue-grey"><i class="fa fa-calendar fa-fw w3-margin-right"></i>Nov. 2023 - Nov. 2024</h6>
              <ul class="edu-ul">
                <li><b>Supervisor</b>: Prof. <a href='https://guosheng.github.io/'>Guosheng Lin</a>
                <li><b>Research Topics</b>: Autonomous Driving, Camera-Radar/Lidar Multi-modal Fusion
                <!-- Merit Student, Outstanding Graduates -->
              </ul>
            </div>
          </div>
        </div>
        <hr>
        <div class="w3-container">
          <div class="pub-figure" style="text-align: center;">
            <img src="./img/adobe_logo_standard_jpg.jpg" style="width:70%; margin:auto;">
          </div>
          <div class="pub-text">
            <div class="info">
              <h5 class="w3-opacity"><b>Research Student, Adobe Research</b></h5>
              <h6 class="w3-text-blue-grey"><i class="fa fa-calendar fa-fw w3-margin-right"></i>APR. 2022 - AUG. 2024</h6>
              <ul class="edu-ul">
                <li><b>Supervisor</b>: Dr. <a href='https://jimmie33.github.io/'>Jianming Zhang</a>
                <li><b>Research Topics</b>: Consistent Video Depth Estimation, In-the-wild Video Depth Dataset
                <!-- Merit Student, Outstanding Graduates -->
              </ul>
            </div>
          </div>
        </div>
        <hr>
        <div class="w3-container">
          <div class="pub-figure" style="text-align: center;">
            <img src="./img/llls.jpg" style="width:90%; margin:auto;">
          </div>
          <div class="pub-text">
            <div class="info">
              <h5 class="w3-opacity"><b>Research Intern (Tencent Project UP 青云计划), Tencent AI Lab</b></h5>
              <h6 class="w3-text-blue-grey"><i class="fa fa-calendar fa-fw w3-margin-right"></i>Feb. 2025 - Aug. 2025</h6>
              <ul class="edu-ul">
                <li><b>Supervisor</b>: Dr. <a href='https://xiaohangzhan.github.io/'>Xiaohang Zhan</a>, Prof. <a href='https://scholar.google.com/citations?user=tMY31_gAAAAJ&hl=zh-CN&oi=ao'>Dong Yu</a>
                <li><b>Research Topics</b>: Multimodal Understanding, Vision-Language Model
                <!-- Merit Student, Outstanding Graduates -->
              </ul>
            </div>
          </div>
        </div>
        <br>
        <br>
      </div>


      
      <div class="w3-container w3-card w3-white w3-margin-bottom">
        <h2 class="w3-text-grey w3-padding-16"><i class="fa fa-book fa-fw w3-margin-right w3-xxlarge w3-text-blue-grey"></i>Publications</h2>
        <div class="w3-container">
          <div class="pub-figure">
            <img src="./img/taco.png">
          </div>
          <div class="pub-text">
            <div class="info">
              <p class="title">TacoDepth: Towards Efficient Radar-Camera Depth Estimation with One-stage Fusion</p>
              <p class="authors"><b>Yiran Wang<sup>*</sup></b>, Jiaqi Li<sup>*</sup>,Chaoyi Hong,Ruibo Li,Liusheng Sun,Xiao Song,Zhe Wang,Zhiguo Cao,Guosheng Lin<br>
              (* Equal contribution)</p>
              <p class="conference">The IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2025</p>
              <p class="Type"><b>Best Paper Award Candidate, Oral Presentation</b> (15/13008, 0.1%)</p>
            </div>
            <div class="link">
              <a href="https://arxiv.org/abs/2504.11773" target="_blank">Paper</a> &nbsp;|&nbsp;
              <a href="https://github.com/RaymondWang987/TacoDepth" target="_blank">GitHub</a> &nbsp;|&nbsp;
              <a href="https://www.youtube.com/watch?v=AdU_GydugYI" target="_blank">YouTube</a> &nbsp;|&nbsp;
              <a href="https://www.bilibili.com/video/BV1n3LyzmE93/?spm_id_from=333.1365.list.card_archive.click&vd_source=806e94b96ef6755e55a2da337c69df47" target="_blank">Bilibili</a>
            </div>
          </div>
        </div>
        <hr>
        <div class="w3-container">
          <div class="pub-figure">
            <img src="./img/ch3.png">
          </div>
          <div class="pub-text">
            <div class="info">
              <p class="title">CH3Depth: Efficient and Flexible Depth Foundation Model with Flow Matching</p>
              <p class="authors">Jiaqi Li<sup>*</sup>, <b>Yiran Wang<sup>*</sup></b>, Jinghong Zheng, Junrui Zhang, Liao Shen, Tianqi Liu, Zhiguo Cao <br>
              (* Equal contribution)</p>
              <p class="conference">The IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2025</p>
              <p class="Type"><b>Highlight</b> (387/13008, 2.9%)</p>
            </div>
            <div class="link">
              <a href="https://openaccess.thecvf.com/content/CVPR2025/papers/Li_CH3Depth_Efficient_and_Flexible_Depth_Foundation_Model_with_Flow_Matching_CVPR_2025_paper.pdf" target="_blank">Paper</a> &nbsp;|&nbsp;
              <a href="https://github.com/lijia7/CH3Depth" target="_blank">GitHub</a>
            </div>
          </div>
        </div>
        <hr>
        <div class="w3-container">
          <div class="pub-figure">
            <img src="./img/nvds2.png">
          </div>
          <div class="pub-text">
            <div class="info">
              <p class="title">NVDS+: Towards Efficient and Versatile Neural Stabilizer for Video Depth Estimation</p>
              <p class="authors"><b>Yiran Wang</b>, Min Shi, Jiaqi Li, Chaoyi Hong, Zihao Huang, Juewen Peng, Zhiguo Cao, Jianming Zhang, Ke Xian, Guosheng Lin</p> 
              <p class="conference">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2024</p>
            </div>
            <div class="link">
              <a href="https://ieeexplore.ieee.org/document/10707178" target="_blank">Paper</a> &nbsp;|&nbsp;
              <a href="https://arxiv.org/pdf/2307.08695" target="_blank">Arxiv</a> &nbsp;|&nbsp;
              <a href="https://github.com/RaymondWang987/NVDS" target="_blank">GitHub</a> &nbsp;|&nbsp;
              <a href="https://www.youtube.com/watch?v=L-yeR_aki20" target="_blank">Video</a>
            </div>
          </div>
        </div>
        <hr>
        <div class="w3-container">
          <div class="pub-figure">
            <img src="./img/sddr.png">
          </div>
          <div class="pub-text">
            <div class="info">
              <p class="title">Self-Distilled Depth Refinement with Noisy Poisson Fusion</p>
              <p class="authors">Jiaqi Li<sup>*</sup>, <b>Yiran Wang<sup>*</sup></b>, Jinghong Zheng, Zihao Huang, Ke Xian, Zhiguo Cao, Jianming Zhang <br>
              (* Equal contribution)</p>
              <p class="conference">The Annual Conference on Neural Information Processing Systems (NeurIPS), 2024</p>
            </div>
            <div class="link">
              <a href="https://arxiv.org/abs/2409.17880" target="_blank">Paper</a> &nbsp;|&nbsp;
              <a href="https://github.com/lijia7/SDDR/" target="_blank">GitHub</a>
            </div>
          </div>
        </div>
        <hr>
        <div class="w3-container">
          <div class="pub-figure">
            <img src="./img/NVDS.png">
          </div>
          <div class="pub-text">
            <div class="info">
              <p class="title">Neural Video Depth Stabilizer</p>
              <p class="authors"><b>Yiran Wang</b>, Min Shi, Jiaqi Li, Zihao Huang, Zhiguo Cao, Jianming Zhang, Ke Xian, Guosheng Lin</p> 
              <p class="conference">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2023</p>
            </div>
            <div class="link">
              <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_Neural_Video_Depth_Stabilizer_ICCV_2023_paper.pdf" target="_blank">Paper</a> &nbsp;|&nbsp;
              <a href="https://github.com/RaymondWang987/NVDS" target="_blank">GitHub</a> &nbsp;|&nbsp;
              <a href="https://raymondwang987.github.io/NVDS/" target="_blank">Project</a>
            </div>
          </div>
        </div>
        <hr>
        <div class="w3-container">
          <div class="pub-figure">
            <img src="./img/MM23-2.png">
          </div>
          <div class="pub-text">
            <div class="info">
              <p class="title">Diffusion-Augmented Depth Prediction with Sparse Annotations</p>
              <p class="authors">Jiaqi Li, <b>Yiran Wang</b>*, Zihao Huang, Jinghong Zheng, Ke Xian, Zhiguo Cao, Jianming Zhang <br>
              (* Corresponding Author)</p>
              <p class="conference">Proceedings of the 31st ACM International Conference on Multimedia (ACM MM), 2023</p>
            </div>
            <div class="link">
              <a href="https://arxiv.org/pdf/2308.02283" target="_blank">Paper</a> &nbsp;|&nbsp;
              <a href="https://dl.acm.org/doi/abs/10.1145/3581783.3611807" target="_blank">Arxiv</a>
            </div>
          </div>
        </div>
        <hr>
        <div class="w3-container">
          <div class="pub-figure">
            <img src="./img/FMNet.png">
          </div>
          <div class="pub-text">
            <div class="info">
              <p class="title">Less is More: Consistent Video Depth Estimation with Masked Frames Modeling</p>
              <p class="authors"><b>Yiran Wang</b>, Zhiyu Pan, Xingyi Li, Zhiguo Cao, Ke Xian, Jianming Zhang</p>
              <p class="conference">Proceedings of the 30th ACM International Conference on Multimedia (ACM MM), 2022</p>
            </div>
            <div class="link">
              <a href="https://arxiv.org/pdf/2208.00380" target="_blank">Paper</a> &nbsp;|&nbsp;
              <a href="https://github.com/RaymondWang987/FMNet" target="_blank">GitHub</a> &nbsp;|&nbsp;
              <a href="https://github.com/RaymondWang987/FMNet" target="_blank">Project</a>
            </div>
          </div>
        </div>
        <hr>
        <div class="w3-container">
          <div class="pub-figure">
              <img src="./img/lxy.png">
          </div>
          <div class="pub-text">
            <div class="info">
              <p class="title">Symmnerf: Learning to explore symmetry prior for single-view view synthesis</p>
              <p class="authors">Xingyi Li, Chaoyi Hong, <b>Yiran Wang</b>, Zhiguo Cao, Ke Xian, Guosheng Lin</p>
              <p class="conference">Proceedings of the Asian conference on computer vision (ACCV), 2022 (<b>Oral</b>)</p>
            </div>
            <div class="link">
              <a href="https://openaccess.thecvf.com/content/ACCV2022/papers/Li_SymmNeRF_Learning_to_Explore_Symmetry_Prior_for_Single-View_View_Synthesis_ACCV_2022_paper.pdf" target="_blank">Paper</a> &nbsp;|&nbsp;
              <a href="https://github.com/xingyi-li/SymmNeRF" target="_blank">GitHub</a> &nbsp;|&nbsp;
              <a href="https://github.com/xingyi-li/SymmNeRF" target="_blank">Project</a>
            </div>
          </div>
        </div>
        <hr>
        <div class="w3-container">
          <div class="pub-figure">
            <img src="./img/ws.png">
          </div>
          <div class="pub-text">
            <div class="info">
              <p class="title">Knowledge distillation for fast and accurate monocular depth estimation on mobile devices</p>
              <p class="authors"><b>Yiran Wang</b>, Xingyi Li, Min Shi, Ke Xian, Zhiguo Cao</p>
              <p class="conference">IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), 2021</p>
            </div>
            <div class="link">
              <a href="https://openaccess.thecvf.com/content/CVPR2021W/MAI/papers/Wang_Knowledge_Distillation_for_Fast_and_Accurate_Monocular_Depth_Estimation_on_CVPRW_2021_paper.pdf" target="_blank">Paper</a>
            </div>
          </div>
        </div>
        <hr>
        <div class="w3-container">
          <div class="pub-figure">
            <img src="./img/11.png">
          </div>
          <div class="pub-text">
            <div class="info">
              <p class="title">Towards Robust Monocular Depth Estimation in Non-Lambertian Surfaces</p>
              <p class="authors">Junrui Zhang, Jiaqi Li, Yachuan Huang, <b>Yiran Wang</b>, Jinghong Zheng, Liao Shen, Zhiguo Cao</p>
              <p class="conference">European Conference on Computer Vision Workshops (ECCVW), 2024</p>
            </div>
            <div class="link">
              <a href="https://arxiv.org/abs/2408.06083" target="_blank">Paper</a>
            </div>
          </div>
        </div>
        <hr>
        <div class="w3-container">
          <div class="pub-figure">
              <img src="./img/NTIRE.png">
          </div>
          <div class="pub-text">
            <div class="info">
              <p class="title">NTIRE 2024 challenge on HR depth from images of specular and transparent surfaces</p>
              <p class="authors">Pierluigi Ramir, Fabio Tosi, Luigi Stefa, Radu Timofte, Alex Costanzino, Matteo Poggi, Samuele Salti, Stefano Matto, Yangyang Zhang, Cailin Wu, Zhuangda He, Shuangshuang Yin, Jiaxu Dong, Yangchenxu Liu, Hao Jiang, Jun Shi, Yixiang Jin, Dingzhe Li, Bingxin Ke, Anton Obukhov, Tinafu Wang, Nando Metzger, Shengyu Huang, Konrad Schindler, Yachuan Huang, Jiaqi Li, Junrui Zhang, <b>Yiran Wang</b>, et al.<br></p>
              <p class="conference">IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), 2023</p>
            </div>
            <div class="link">
              <a href="https://openaccess.thecvf.com/content/CVPR2024W/NTIRE/papers/Ramirez_NTIRE_2024_Challenge_on_HR_Depth_from_Images_of_Specular_CVPRW_2024_paper.pdf" target="_blank">Paper</a> &nbsp;|&nbsp;
              <a href="https://cvlai.net/ntire/2024/" target="_blank">Website</a>
            </div>
          </div>
        </div>
        <hr>
        <div class="w3-container">
          <div class="pub-figure">
            <img src="./img/MAi.png">
          </div>
          <div class="pub-text">
            <div class="info">
              <p class="title">Fast and accurate single-image depth estimation on mobile devices, mobile ai 2021 challenge: Report</p>
              <p class="authors">Andrey Ignatov, Grigory Malivenko, David Plowman, Samarth Shukla, Radu Timofte, Ziyu Zhang, Yicheng Wang, Zilong Huang, Guozhong Luo, Gang Yu, Bin Fu, <b>Yiran Wang</b>, et al.<br></p>
              <p class="conference">IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), 2021</p>
            </div>
            <div class="link">
              <a href="https://openaccess.thecvf.com/content/CVPR2021W/MAI/papers/Ignatov_Fast_and_Accurate_Single-Image_Depth_Estimation_on_Mobile_Devices_Mobile_CVPRW_2021_paper.pdf" target="_blank">Paper</a> &nbsp;|&nbsp;
              <a href="https://ai-benchmark.com/workshops/mai/2021/" target="_blank">Website</a>
            </div>
          </div>
        </div>
        <br>
        <br>
      </div>

      <div class="w3-container w3-card w3-white w3-margin-bottom">
        <h2 class="w3-text-grey w3-padding-16"><i class="fa fa-trophy fa-fw w3-margin-right w3-xxlarge w3-text-blue-grey"></i>Competitions and Awards</h2>
        <div class="w3-container">
          <!-- <h5 class="w3-opacity"><b>Front End Developer / w3schools.com</b></h5> -->
          <div class="pub-figure">
            <img src="./img/nntire.png">
          </div>
          <div class="pub-text">
            <p class="title"><b>Winner Award</b> in "CVPR 2024 NTIRE Challenge on HR Depth from Images of Specular and Transparent Surfaces".</p>
            <p class="authors">Yachuan Huang, Jiaqi Li, Junrui Zhang, <b>Yiran Wang</b>, Zihao Huang, Tianqi Liu, Zhiguo Cao</p>
          </div>
        </div>
        <hr>
        <div class="w3-container">
          <!-- <h5 class="w3-opacity"><b>Front End Developer / w3schools.com</b></h5> -->
          <div class="pub-figure">
            <img src="./img/mmai.png">
          </div>
          <div class="pub-text">
            <p class="title"><b>Runner-Up Award</b> in "CVPR 2021 Mobile AI Single-Image Depth Estimation Challenge".</p>
            <p class="authors"><b>Yiran Wang</b>, Xingyi Li, Min Shi, Ke Xian, Zhiguo Cao</p>
          </div>
        </div>
        <hr>
        <div class="w3-container">
          <!-- <h5 class="w3-opacity"><b>Front End Developer / w3schools.com</b></h5> -->
          <div class="pub-figure">
            <img src="./img/tricky.jpg">
          </div>
          <div class="pub-text">
            <p class="title"><b>Winner Award</b> in "ECCV 2024 TRICKY Challenge on Monocular Depth from Images of Specular and Transparent Surfaces".</p>
            <p class="authors">Jiaqi Li, Yachuan Huang, Junrui Zhang, <b>Yiran Wang</b>, Jinghong Zheng, Liao Shen, Zhiguo Cao</p>
          </div>
        </div>
         <hr>
        <div class="w3-container">
          <!-- <h5 class="w3-opacity"><b>Front End Developer / w3schools.com</b></h5> -->
          <div class="pub-figure">
            <img src="./img/guojiang.jpg">
          </div>
          <div class="pub-text">
            <p class="title"><b>National Scholarship</b> for Doctoral Students in 2024.</p>
            <p class="authors"><b>Rank first</b> among all candidates in the school of AIA, HUST</p>
          </div>
        </div>
        <hr>
        <div class="w3-container">
          <!-- <h5 class="w3-opacity"><b>Front End Developer / w3schools.com</b></h5> -->
          <div class="pub-figure">
            <img src="./img/guojiang.jpg">
          </div>
          <div class="pub-text">
            <p class="title"><b>National Scholarship</b> for Doctoral Students in 2025.</p>
            <p class="authors">the National Scholarship for Doctoral Students for <b>two consecutive years</b></p>
          </div>
        </div>
        <br>
        <br>
      </div>

      <div class="w3-container w3-card w3-white">
        <h2 class="w3-text-grey w3-padding-16"><i class="fa fa-suitcase fa-fw w3-margin-right w3-xxlarge w3-text-blue-grey"></i>Other Experiences</h2>
        <div class="w3-container">
          <h6 class="w3-text-blue-grey"><i class="fa fa-calendar fa-fw w3-margin-right"></i>Oct. 2021 - Oct. 2022</h6> 
          <p>Worked on project “Novel View Synthesis and 3D Video Conversion”.</p>
          <hr>
          <h6 class="w3-text-blue-grey"><i class="fa fa-calendar fa-fw w3-margin-right"></i>Jan. 2023 - Aug. 2024</h6> 
          <p>Worked on project "Efficient Depth Super-resolution and Video Depth Estimation".</p><br>
          <h6 class="w3-text-blue-grey"><i class="fa fa-calendar fa-fw w3-margin-right"></i>Jul. 2019 - Sep. 2019</h6> 
          <p>Worked as the student project leader of NUS AI Summer Camp in Singapore.</p><br>
        </div>
        <br>
      </div>

      <div style="clear:both;">
        <p align="right">Last Updated on 16th October, 2025</p>
        <p align="right">Published with <a href='https://pages.github.com/'>GitHub Pages</p>
      </div>
      <hr>
      <script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=9C9xIgfCsHmCLj-Z6NcR4Jp7lQEN6MZw64UiHxtA4hE&cl=ffffff&w=a"></script>

      

      

    <!-- End Right Column -->
    </div>
    
  <!-- End Grid -->
  </div>
  
  <!-- End Page Container -->
</div>

<!-- Footer. This section contains an ad for W3Schools Spaces. You can leave it to support us. -->
<footer class="w3-container w3-blue-grey w3-center w3-margin-top">
  <!-- <p>"Somebody has to win, so why not be me?"</p>
  <p>— Kobe Bryant</p> -->
  <p>Huazhong University of Science and Technology, &nbsp;Yiran Wang</p>
</footer>


</body>
</html>
